import json
import uuid
import logging
import os
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI
from qdrant_client import QdrantClient, models as qm
from sklearn.feature_extraction.text import HashingVectorizer

# -------------------------
# ‚öôÔ∏è CONFIGURATION
# -------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("üö® Cl√© API OpenAI manquante ! V√©rifie ton fichier .env")

COLLECTION_NAME = "bofip_hybrid"
CHUNKS_FILE = "data/processed/bofip_small_chunks.json"
QDRANT_URL = "http://localhost:6333"
BATCH_SIZE = 500
VECTOR_SIZE = 1536  # text-embedding-3-small
LOG_FILE = "logs/indexation_openai.log"

# -------------------------
# ü™µ LOGGING
# -------------------------
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, mode="w", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
log = logging.getLogger(__name__)

# -------------------------
# üîå INITIALISATION DES CLIENTS
# -------------------------
client_openai = OpenAI(api_key=OPENAI_API_KEY)
client_qdrant = QdrantClient(url=QDRANT_URL)
hv = HashingVectorizer(n_features=2**16, alternate_sign=False, norm=None)

# -------------------------
# üß™ TEST DE CONNEXION OPENAI
# -------------------------
try:
    test = client_openai.embeddings.create(model="text-embedding-3-small", input=["test"])
    log.info("‚úÖ Connexion OpenAI r√©ussie ‚Äî mod√®le accessible.")
except Exception as e:
    log.error(f"üö® Erreur lors de la connexion √† OpenAI : {e}")
    raise SystemExit(1)

# -------------------------
# üß± COLLECTION QDRANT
# -------------------------
collections = [c.name for c in client_qdrant.get_collections().collections]
if COLLECTION_NAME not in collections:
    log.info(f"Cr√©ation de la collection {COLLECTION_NAME}...")
    client_qdrant.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config={"dense": qm.VectorParams(size=VECTOR_SIZE, distance=qm.Distance.COSINE)},
        sparse_vectors_config={"sparse": qm.SparseVectorParams()},
    )
else:
    log.info(f"Collection existante trouv√©e : {COLLECTION_NAME}")

# -------------------------
# üìÑ LECTURE DES CHUNKS
# -------------------------
if not os.path.exists(CHUNKS_FILE):
    raise FileNotFoundError(f"‚ùå Fichier introuvable : {CHUNKS_FILE}")

log.info(f"üìÑ Lecture des chunks depuis {CHUNKS_FILE}...")
with open(CHUNKS_FILE, "r", encoding="utf-8") as f:
    chunks = json.load(f)

texts = [chunk.get("contenu", "") for chunk in chunks]
log.info(f"üìä {len(texts):,} chunks d√©tect√©s ‚Äî d√©but de l‚Äôindexation")

# -------------------------
# üß† EMBEDDINGS OPENAI
# -------------------------
def embed_openai(batch_texts):
    """Cr√©e des embeddings via l‚ÄôAPI OpenAI (text-embedding-3-small)."""
    response = client_openai.embeddings.create(
        model="text-embedding-3-small",
        input=batch_texts
    )
    return [d.embedding for d in response.data]

# -------------------------
# ü™∂ ENCODAGE SPARSE
# -------------------------
log.info("ü™∂ G√©n√©ration des embeddings sparse (HashingVectorizer)...")
X_sparse = hv.transform(texts)
indices_list = [X_sparse[i].indices.tolist() for i in range(X_sparse.shape[0])]
values_list = [X_sparse[i].data.tolist() for i in range(X_sparse.shape[0])]

# -------------------------
# üì§ INDEXATION DANS QDRANT
# -------------------------
log.info("üöÄ D√©but de l‚Äôinsertion dans Qdrant...")

for start in tqdm(range(0, len(texts), BATCH_SIZE), desc="Indexation Qdrant"):
    end = min(start + BATCH_SIZE, len(texts))
    batch_texts = texts[start:end]

    # G√©n√©ration des embeddings OpenAI par lot
    dense_vectors = embed_openai(batch_texts)

    # Construction des points √† ins√©rer
    points = [
        qm.PointStruct(
            id=str(uuid.uuid4()),
            vector={
                "dense": dense_vectors[i],
                "sparse": qm.SparseVector(
                    indices=indices_list[start + i],
                    values=values_list[start + i],
                ),
            },
            payload={"text": batch_texts[i], **chunks[start + i].get("metadata", {})},
        )
        for i in range(len(batch_texts))
    ]

    # Insertion dans Qdrant
    client_qdrant.upsert(collection_name=COLLECTION_NAME, points=points)
    log.info(f"üì¶ Batch {start//BATCH_SIZE + 1}: {len(points)} points ins√©r√©s ({end}/{len(texts)})")

log.info(f"‚úÖ Indexation termin√©e ‚Äî {len(texts):,} points ins√©r√©s dans {COLLECTION_NAME}")

