# -*- coding: utf-8 -*-
"""
answerer.py ‚Äî G√©n√©ration finale de la r√©ponse (prompt multi-√©tape conserv√©)
"""

import logging
from openai import OpenAI
from rag.retrieval.qdrant_retriever import QdrantRetriever
from rag.config import (
    OPENAI_API_KEY,
    DEFAULT_CHAT_MODEL,
    PROMPT_ANSWER,
)

# ==========================================================
# Configuration du logger
# ==========================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)


# ==========================================================
# Fonction principale
# ==========================================================
def generate_answer(question, small_chunks, include_sources=True, llm_model=None):
    """
    G√©n√®re la r√©ponse finale √† partir de la question et des extraits pertinents.
    Conserve la logique multi-√©tape du prompt original.
    """
    logging.info("üöÄ D√©but de generate_answer()")

    # --- V√©rification / fallback du mod√®le ---
    if not llm_model:
        llm_model = DEFAULT_CHAT_MODEL
        logging.warning("‚ö†Ô∏è Aucun mod√®le LLM fourni ‚Äî fallback sur %s", llm_model)
    else:
        logging.info(f"üß† Mod√®le s√©lectionn√© : {llm_model}")

    # --- Initialisation des clients ---
    client = OpenAI(api_key=OPENAI_API_KEY)
    retriever = QdrantRetriever(chat_model=llm_model)

    # --- √âtape 1 : R√©cup√©ration des big_chunks ---
    try:
        logging.info("üì• R√©cup√©ration des BigChunks depuis Qdrant...")
        big_chunks = retriever.retrieve_big_chunks(small_chunks)
        logging.info(f"‚úÖ {len(big_chunks)} BigChunks r√©cup√©r√©s.")
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des BigChunks : {e}")
        big_chunks = small_chunks  # fallback minimal
        logging.info("‚û°Ô∏è Utilisation des small_chunks comme fallback.")

    # --- √âtape 2 : Construction du contexte ---
    context = "\n\n---\n\n".join(
        ch.get("text")
        or ch.get("content")
        or ch.get("metadata", {}).get("text")
        or ""
        for ch in big_chunks
        if ch
    )
    logging.info(f"üìö Contexte construit ({len(context)} caract√®res).")

    # --- √âtape 3 : Construction du prompt multi-√©tape ---
    user_prompt = (
        f"Voici la question : \n\n{question}\n\n"
        f"Voici des extraits juridiques avec leur source issus de bases documentaires :\n\n{context}\n\n"
        f"** 1√®re √©tape ** : En fonction de la question pos√©e et des extraits juridiques, "
        f"s√©lectionne ceux qui r√©pondent totalement ou partiellement √† la question.\n"
        f"** 2√®me √©tape ** : V√©rifie que les r√©ponses s√©lectionn√©es sont coh√©rentes entre elles.\n"
        f"Si tu n‚Äôas pas assez d‚Äô√©l√©ments, r√©ponds : "
        f"\"je n‚Äôai pas assez d‚Äô√©l√©ments √† ma disposition pour r√©pondre\".\n"
        f"** 3√®me √©tape ** : Formule un r√©sum√© complet des textes en citant les sources.\n"
        f"** 4√®me √©tape ** : Explique comment ces textes et uniquement ces textes "
        f"s‚Äôappliquent concr√®tement √† la question."
    )
    logging.info("üß© Prompt multi-√©tape construit avec succ√®s.")

    # --- √âtape 4 : Appel au mod√®le ---
    try:
        logging.info("üì® Envoi du prompt au mod√®le OpenAI...")
        response = client.chat.completions.create(
            model=llm_model,
            messages=[
                {"role": "system", "content": PROMPT_ANSWER},
                {"role": "user", "content": user_prompt.strip()},
            ],
            temperature=0.0,
        )
        answer = response.choices[0].message.content.strip()
        logging.info("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s.")
    except Exception as e:
        logging.exception(f"üí• Erreur lors de l‚Äôappel au mod√®le : {e}")
        raise  # on laisse remonter pour voir dans Streamlit

    # --- √âtape 5 : Ajout des sources ---
    if include_sources:
        try:
            seen, sources = set(), []
            for ch in big_chunks:
                md = ch.get("metadata", {}) or {}
                url = md.get("url") or md.get("permalien")
                if not url or url in seen:
                    continue
                seen.add(url)
                titre = md.get("titre_document") or "Source inconnue"
                sources.append(f"- [{titre}]({url})")

            if sources:
                answer += "\n\n---\nüìé **Sources utilis√©es :**\n" + "\n".join(sources)
                logging.info(f"üìé {len(sources)} sources ajout√©es.")
            else:
                logging.info("‚ÑπÔ∏è Aucune source √† ajouter.")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è √âchec lors de l‚Äôajout des sources : {e}")

    logging.info("üèÅ Fin de generate_answer()")
    return answer
