import logging
from openai import OpenAI
from rag.retrieval.qdrant_retriever import QdrantRetriever
from rag.config import PROMPT_ANSWER

# Configuration de base du logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


def generate_answer(question, small_chunks, include_sources=True, llm_model=None):
    logging.info("üöÄ D√©but de generate_answer()")

    # --- V√©rification du mod√®le ---
    if not llm_model:
        logging.error("‚ùå Aucun mod√®le LLM n‚Äôa √©t√© sp√©cifi√©. V√©rifie l‚ÄôUI Streamlit.")
        raise ValueError("Aucun mod√®le LLM n‚Äôa √©t√© sp√©cifi√©.")

    logging.info(f"üß† Mod√®le s√©lectionn√© : {llm_model}")

    client = OpenAI()
    retriever = QdrantRetriever()

    # --- √âtape 1 : R√©cup√©ration des big_chunks ---
    try:
        logging.info("üì• R√©cup√©ration des BigChunks depuis Qdrant...")
        big_chunks = retriever.get_big_chunks_from_small(small_chunks)
        logging.info(f"‚úÖ {len(big_chunks)} BigChunks r√©cup√©r√©s.")
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Erreur Qdrant : {e}")
        big_chunks = small_chunks  # fallback minimal
        logging.info("‚û°Ô∏è Utilisation des small_chunks comme fallback.")

    # --- √âtape 2 : Construction du contexte ---
    context = "\n\n---\n\n".join(
        ch.get("text") or ch.get("content") or ch.get("metadata", {}).get("text") or ""
        for ch in big_chunks
        if ch
    )
    logging.info(f"üìö Contexte construit ({len(context)} caract√®res).")

    # --- √âtape 3 : Construction du prompt ---
    user_prompt = (
        f"Voici la question : \n\n{question}\n\n"
        f"Voici des extraits juridiques avec leur source issus de bases documentaires :\n\n{context}\n\n"
        f"** 1√®re √©tape ** : En fonction de la question pos√©e et des extraits juridiques, "
        f"s√©lectionne ceux qui r√©pondent totalement ou partiellement √† la question.\n"
        f"** 2√®me √©tape ** : V√©rifie que les r√©ponses s√©lectionn√©es sont coh√©rentes entre elles.\n"
        f"Si tu n‚Äôas pas assez d‚Äô√©l√©ments, r√©ponds : "
        f"\"je n‚Äôai pas assez d‚Äô√©l√©ments √† ma disposition pour r√©pondre\".\n"
        f"** 3√®me √©tape ** : Formule un r√©sum√© complet des textes en citant les sources.\n"
        f"** 4√®me √©tape ** : Explique comment ces textes et uniquement ces textes "
        f"s‚Äôappliquent concr√®tement √† la question."
    )
    logging.info("üß© Prompt construit avec succ√®s.")

    # --- √âtape 4 : Appel au mod√®le ---
    try:
        logging.info("üì® Envoi du prompt au mod√®le OpenAI...")
        response = client.chat.completions.create(
            model=llm_model,
            messages=[
                {"role": "system", "content": PROMPT_ANSWER},
                {"role": "user", "content": user_prompt.strip()},
            ],
            temperature=0.0,
        )
        logging.info("‚úÖ R√©ponse re√ßue du mod√®le.")
        answer = response.choices[0].message.content.strip()
    except Exception as e:
        logging.exception(f"üí• Erreur lors de l‚Äôappel au mod√®le : {e}")
        raise  # On laisse remonter l‚Äôerreur pour la voir dans Streamlit

    # --- √âtape 5 : Ajout des sources ---
    if include_sources:
        try:
            seen, sources = set(), []
            for ch in big_chunks:
                md = ch.get("metadata", {}) or {}
                url = md.get("url") or md.get("permalien")
                if not url or url in seen:
                    continue
                seen.add(url)
                titre = md.get("titre_document") or "Source inconnue"
                sources.append(f"- [{titre}]({url})")
            if sources:
                answer += "\n\n---\nüìé **Sources utilis√©es :**\n" + "\n".join(sources)
            logging.info(f"üìé {len(sources)} sources ajout√©es.")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è √âchec lors de l‚Äôajout des sources : {e}")

    logging.info("üèÅ Fin de generate_answer()")
    return answer
