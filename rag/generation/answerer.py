import os
from openai import OpenAI
from dotenv import load_dotenv
from rag.config import OPENAI_API_KEY
from rag.config import PROMPT_ANSWER
from rag.config import OPENAI_CHAT_MODEL
from rag.retrieval.retriever import get_big_chunks_from_small

# === CHARGEMENT CLE API ===

client = OpenAI(api_key=OPENAI_API_KEY)

# === AJOUT DES SOURCES EN ANNEXE===

def append_sources(answer, chunks):

    sources = set()  # Utilise un set pour √©viter les doublons de sources
    for chunk in chunks:
        meta = chunk.get("metadata", {})  # R√©cup√®re les m√©tadonn√©es associ√©es au chunk
        titre = meta.get("titre_document", "Sans titre")  # Titre du document (fallback si manquant)
        base = meta.get("source", "source inconnue").upper()  # Nom de la base (ex: BOFiP), en majuscules
        url = meta.get("permalien", None)  # Lien vers la source juridique

        if url:  # Si un permalien est disponible, on ajoute une entr√©e format√©e
            sources.add(f"- [{titre} ‚Äî {base}]({url})")  # Format Markdown : lien cliquable

    if sources:  # S'il y a au moins une source, on les ajoute √† la r√©ponse
        answer += "\n\nüìé **Sources utilis√©es :**\n" + "\n".join(sorted(sources))

    return answer

# === ENVOI DU CONTEXTE AU LLM ET FORMULATION DE LA REPONSE===

def generate_answer(query, chunks, include_sources=True):
    print("\nüß† √âtape : G√©n√©ration de la r√©ponse finale")

    context = "\n\n---\n\n".join(
        f"{chunk.get('metadata', {}).get('titre_document', '')}\n"
        f"{chunk.get('metadata', {}).get('titre_bloc', '')}\n"
        f"Source : {chunk.get('metadata', {}).get('permalien', 'Source inconnue')}\n\n"
        f"{chunk.get('contenu', '')}"
        for chunk in chunks
    )

    user_prompt = (
        f"Voici la question : \n\n{query}\n\n"
        f"Voici des extraits juridiques avec leur source issus de bases documentaires :\n\n{context}\n\n"
        f"** 1√®re √©tape ** : En fonction de la question pos√©e et des extraits juridiques, s√©lectionne ceux qui r√©pondent totalement ou partiellement √† la question.\n"
        f"Format attendu : ne doit pas apparaitre dans la r√©ponse.\n"
        f"** 2√®me √©tape ** : V√©rifie que les r√©ponses s√©lectionn√©es sont coh√©rentes entre elles.\n"
        f"Format attendu : ne doit pas apparraitre dans la r√©ponse. \n"
        f"Si tu n'as pas assez d'√©l√©ments, r√©ponds : \"je n'ai pas assez d'√©l√©ments √† ma disposition pour r√©pondre\" et ne passe pas aux √©tapes suivantes.\n"
        f"** 3√®me √©tape ** : Formule un r√©sum√© compl√®te des textes en citant les sources.\n"
        f"Format attendu : Sous-partie : Textes juridiques applicables (en gras et avec une police sup√©rieure) - R√©sum√© de l'article - Source\n"
        f"** 4√®me √©tape ** : Explique comment ces textes et uniquement ces texte s‚Äôappliquent concr√®tement √† la question. Tu ne dois pas extrapoler\n"
        f"Format attendu : Sous-partie : Application au cas d‚Äôesp√®ce (en gras et avec une police sup√©rieure)"
    )

    print("\nüì® Prompt envoy√© au LLM :\n")
    print(user_prompt)

    response = client.chat.completions.create(
        model=OPENAI_CHAT_MODEL,
        messages=[
            {"role": "system", "content": PROMPT_ANSWER},
            {"role": "user", "content": user_prompt}
        ]
    )
    return response.choices[0].message.content.strip()