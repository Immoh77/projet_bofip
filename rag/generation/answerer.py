import os
from openai import OpenAI
from rag.config import PROMPT_ANSWER, OPENAI_CHAT_MODEL
from rag.retrieval.qdrant_retriever import QdrantRetriever


client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# -------------------------------
# üîπ 1. Utilitaires de formatage
# -------------------------------

def _norm_text_from_chunk(ch: dict) -> str:
    """R√©cup√®re le texte du chunk (supporte small/big et variantes)."""
    if ch.get("text"):
        return ch["text"]
    if ch.get("contenu"):
        return ch["contenu"]
    meta = ch.get("metadata", {}) or {}
    return meta.get("text", "")


def _build_context_from_big_chunks(big_chunks: list) -> str:
    """
    Construit le contexte √† injecter au LLM √† partir des big chunks.
    Pas de troncature ‚Äî les big chunks sont suppos√©s coh√©rents.
    """
    context_lines = []
    seen = set()

    for ch in big_chunks:
        cid = ch.get("chunk_id") or ch.get("metadata", {}).get("chunk_id")
        if cid in seen:
            continue
        seen.add(cid)

        meta = ch.get("metadata", {}) or {}
        titre = meta.get("titre_document", "Sans titre")
        base = (meta.get("source", "source inconnue") or "").upper()
        url = meta.get("permalien", "Source inconnue")

        texte = _norm_text_from_chunk(ch).strip()
        context_lines.append(
            f"Titre : {titre}\nBase : {base}\nSource : {url}\n\n{texte}\n"
        )

    return "\n".join(context_lines)


def append_sources(answer: str, chunks: list) -> str:
    """Ajoute les sources uniques √† la fin de la r√©ponse."""
    seen, sources = set(), []
    for ch in chunks:
        meta = ch.get("metadata", {}) or {}
        titre = meta.get("titre_document", "Sans titre")
        url = meta.get("permalien", None)
        if not url or url in seen:
            continue
        seen.add(url)
        sources.append(f"- [{titre}]({url})")
    if not sources:
        return answer
    return f"{answer}\n\n---\n\nüìé **Sources utilis√©es :**\n" + "\n".join(sources)


# -------------------------------
# üîπ 2. G√©n√©ration principale
# -------------------------------

def generate_answer(query: str, chunks: list, include_sources: bool = True) -> str:
    """
    G√©n√®re la r√©ponse finale √† partir des small chunks (issus du retriever).
    Le module remonte automatiquement les big chunks pour le contexte.
    """
    # 1Ô∏è‚É£ Remonter aux big chunks associ√©s
    try:
        retriever = QdrantRetriever()
        big_chunks = retriever.get_big_chunks_from_small(chunks)
    except Exception:
        big_chunks = chunks

    # 2Ô∏è‚É£ Construire le contexte complet
    context = _build_context_from_big_chunks(big_chunks)

    # 3Ô∏è‚É£ Prompt structur√© ‚Äî √©tapes de raisonnement
    user_prompt = f"""
Question pos√©e :
{query}

Contexte documentaire :
{context}

---

**1√®re √©tape :** S√©lectionne parmi ces extraits juridiques ceux qui r√©pondent directement ou partiellement √† la question pos√©e.
> Format attendu : ne doit pas appara√Ætre dans la r√©ponse.

**2√®me √©tape :** V√©rifie la coh√©rence des extraits s√©lectionn√©s.  
> Format attendu : ne doit pas appara√Ætre dans la r√©ponse.  
> Si tu n‚Äôas pas assez d‚Äôinformations pour r√©pondre, dis uniquement : "Je n‚Äôai pas assez d‚Äô√©l√©ments en ma possession pour r√©pondre" et arr√™te-toi.

**3√®me √©tape :** R√©sume les textes applicables en citant les sources exactes.  
> Format attendu :  
> - **Textes juridiques applicables** (titre de section en gras et plus grand)  
> - R√©sum√© clair de l‚Äôarticle ou des extraits pertinents  
> - Indique la **source** (ex. BOFiP, Code g√©n√©ral des imp√¥ts, etc.)

**4√®me √©tape :** Explique comment ces textes s‚Äôappliquent concr√®tement √† la question pos√©e, sans extrapolation ni ajout externe.  
> Format attendu :  
> - **Application au cas d‚Äôesp√®ce** (titre en gras et plus grand)
"""

    # 4Ô∏è‚É£ Appel au mod√®le OpenAI
    response = client.chat.completions.create(
        model=OPENAI_CHAT_MODEL,
        messages=[
            {"role": "system", "content": PROMPT_ANSWER},
            {"role": "user", "content": user_prompt.strip()},
        ],
        temperature=0.0,
    )

    answer = response.choices[0].message.content.strip()

    # 5Ô∏è‚É£ Ajouter les sources si demand√©
    if include_sources:
        answer = append_sources(answer, big_chunks)

    return answer
