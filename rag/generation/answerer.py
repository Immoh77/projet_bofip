# -*- coding: utf-8 -*-
"""
answerer.py ‚Äî G√©n√©ration finale de la r√©ponse (prompt multi-√©tape enrichi)
---------------------------------------------------------------------------
Ce module :
1. Prend la question utilisateur et les r√©sultats du retriever (QdrantRetriever)
2. Construit un prompt complet et contextuel √† partir du RAG
3. G√©n√®re une r√©ponse coh√©rente et sourc√©e via OpenAI
4. Peut √™tre ex√©cut√© manuellement pour test
"""

import logging
from openai import OpenAI
from rag.retrieval.qdrant_retriever import QdrantRetriever
from rag.config import (
    OPENAI_API_KEY,
    DEFAULT_CHAT_MODEL,
    PROMPT_ANSWER_HEADER,
    PROMPT_USER_ANSWER,
)

# === Configuration logging (√©pur√©e) ===
logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)

def append_sources_to_context(chunks):
    """
    Construit un texte structur√© avec les m√©tadonn√©es et le contenu de chaque chunk.
    Chaque bloc affiche :
    - Titre du document
    - Source / Base
    - URL (permalien)
    - Contenu complet du chunk (big_chunk)
    """

    formatted = []
    for idx, ch in enumerate(chunks, start=1):
        meta = ch.get("metadata", {}) or {}

        titre = meta.get("titre_document") or meta.get("titre_bloc") or "Sans titre"
        base = (meta.get("base") or meta.get("source") or "Source inconnue").capitalize()
        url = meta.get("permalien") or meta.get("url") or "Non pr√©cis√©e"
        contenu = ch.get("text") or ch.get("contenu") or ""

        bloc = (
            f"=== BIG CHUNK {idx} ===\n"
            f"Titre : {titre}\n"
            f"Base : {base}\n"
            f"Source : {url}\n"
            f"Contenu :\n{contenu.strip()}\n"
        )

        formatted.append(bloc)

    return "\n\n".join(formatted).strip()

# ==========================================================
# Fonction principale de g√©n√©ration de r√©ponse
# ==========================================================
def generate_answer(question, retriever_output, include_sources=True, llm_model=None):
    """
    G√©n√®re la r√©ponse finale √† partir :
    - de la question originale
    - du r√©sultat complet du retriever (question clarifi√©e, sous-questions, big_chunks)
    """
    logger.info("üöÄ D√©but de generate_answer()")

    # --- V√©rification / fallback du mod√®le ---
    if not llm_model:
        llm_model = DEFAULT_CHAT_MODEL
        logger.info(f"üß† Utilisation du mod√®le par d√©faut : {llm_model}")

    # --- Initialisation du client OpenAI ---
    client = OpenAI(api_key=OPENAI_API_KEY)

    # --- Extraction des donn√©es RAG ---
    big_chunks = retriever_output.get("big_chunks_associes", [])
    clarified = retriever_output.get("question_clarifiee", "Non disponible")
    subqs = retriever_output.get("sous_questions", [])

    # --- Construction du contexte enrichi avec m√©tadonn√©es ---
    context = append_sources_to_context(big_chunks)
    logger.info(f"üìö Contexte enrichi construit ({len(context)} caract√®res).")

    # --- Formatage des sous-questions ---
    subqs_text = "\n".join([f"- {s}" for s in subqs]) or "Aucune sous-question g√©n√©r√©e."

    # --- Construction du prompt complet depuis la config ---
    user_prompt = PROMPT_USER_ANSWER.format(
        question_originale=question,
        question_clarifiee=clarified,
        subquestions=subqs_text,
        context=context,
    )
    logger.info("üß© Prompt multi-√©tape (issu de config) construit avec succ√®s.")

    # --- Appel au mod√®le ---

    # --- Log du prompt envoy√© au mod√®le ---
    logger.info("\n" + "=" * 80)
    logger.info("üß† PROMPT ENVOY√â AU LLM")
    logger.info("-" * 80)
    logger.info(user_prompt)
    logger.info("=" * 80 + "\n")

    try:
        logger.info("üì® Envoi du prompt au mod√®le OpenAI...")
        response = client.chat.completions.create(
            model=llm_model,
            messages=[
                {"role": "system",
                 "content": "Tu es un expert-comptable et fiscaliste. R√©ponds de mani√®re claire, concise et sourc√©e aux extraits suivants."},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.4,
        )
        answer = response.choices[0].message.content.strip()
        logger.info("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s.")
    except Exception as e:
        logger.exception(f"üí• Erreur lors de l‚Äôappel au mod√®le : {e}")
        raise

    logger.info("üèÅ Fin de generate_answer()")
    return answer


# ==========================================================
# Ex√©cution manuelle (test en console)
# ==========================================================
if __name__ == "__main__":
    logger.info("üß† Test manuel de g√©n√©ration de r√©ponse.")
    retriever = QdrantRetriever()
    q = input("‚ùì Question : ")
    retrieved = retriever.retrieve_with_subquery_rerank(q)
    final_answer = generate_answer(q, retrieved)
    print("\n=== üßæ R√©ponse finale ===\n")
    print(final_answer)
